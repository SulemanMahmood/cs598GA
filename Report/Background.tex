\section{Background}
In this section, a brief overview of the Worst Case Execution Time (WCET) estimation and approximation in programs is presented.

\subsection{Worst Case Execution Time Estimation}
The general problem of calculating the time required for computation is an undecidable problem i.e. there is no known method to find the time it takes for a computation to complete. The obvious solution of running the program and timing it is not enough because the input to the program that will result in the longest processing time is generally not known and trying all possible inputs infeasible in practice. Timing the program with multiple inputs can give us an estimate of the time it takes for a program to complete but this often results in underestimation. In hard real time system, allocating a time budget based on an underestimate is not acceptable. Several tools (\cite{RapiTime}, \cite{Drops}, \cite{Wenzel}) have been built around the idea of timing basic blocks in the code and combining the timings in novel ways to arrive at a tight estimate. An alternate approach is static analysis of the program. Many static analysis based tools (\cite{AIT}, \cite{BoundT}, \cite{Chronos}) have been developed in the past two decades. Static analysis produces and overestimate of the worst possible running time of a program. The overestimate can be used to design hard real time systems. In this project, a static analysis based tool are used to estimate WCET is used.

Static analysis based tools calculate WCET using the following steps.
\begin{enumerate}
\item Find bounds on the values of variables in the program.
\item Use the bounds to estimate the number of times loops are going to run.
\item Use a model of a processor to estimate time for each basic block.
\item Combine times for basic blocks and loop bounds to formulate a linear constraints.
\item Maximize program time subject to the constraints.
\end{enumerate}

The most difficult part of this process is finding the loop bounds and  modeling the processors. Both of then are challenging because of different reasons. Figuring out loop bound is an undecidable problem. The tools are able to find bounds for some loops but there is no way of knowing bounds for loops which depend on the program input. In such cases, the developers need to annotate the program with information about the loop bounds. These annotations must be tracked through the compilation process. The second challenge comes from the complexity and the large variety of processors available in the market. In the last four decades, advances in processor architecture and fabrication means that deep pipelines, speculative execution and multi-layer memory hierarchy are now common even in low end processors. These features add complexity to the processor model that needs to be considered while performing WCET analysis. 


\subsection{Program Approximations}
The idea of approximate computation has been around since the time people started doing computations. In computer systems, all floating point computations are essentially approximate computations. In the last decade, research (\cite{Loop}, \cite{Image}, \cite{Canary}) have shown that using aggressive approximations in computer programs still produce results that are acceptable for humans. These aggressive approximation reduce the resources required by the computer to perform the computation. 

In this project, the following approximations are used: 
\begin{enumerate}
\item Input Reduction: A small section of the input is processed instead of the complete input.
\item Loop Perforation: Some iterations of the loop are skipped.
\item Under-sampling and Extrapolation: A small sample of the input is processed. The results are scaled to the size of input.
\item Precision Reduction: Using less precise data types which can be processed quickly.
\item Loop Truncation: Loop is truncated after some iterations and results calculated so far are returned.
\item Recursion Limiting: The base case of recursion is changed to limit recursion.
\end{enumerate}

The level of approximation in a program can be controlled by using the so called knobs. The knobs control the aggressiveness of the approximation. All approximations presented in this report except precision reduction have knobs associated with them. The knobs control the following aspects of these approximations.
\begin{enumerate}
\item Input Reduction: Knob controls the size of the input slice to be processed.
\item Loop Perforation: Knob controls how many iterations of the loop can be skipped.
\item Under-sampling and Extrapolation: Knob controls the size of sample taken from input. It also acts as a scaling factor for the output.
\item Loop Truncation: Knob controls how many loop iterations may be skipped.
\item Recursion Limiting: Knob controls the possible number of leaf nodes.
\end{enumerate}

Some frameworks (\cite{Green}, \cite{Knobs}) dynamically determine the approximation knob values to meet energy or accuracy requirements of the systems. This process is called auto tuning.
